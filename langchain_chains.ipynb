{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Langchain Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import dotenv_values\n",
    "\n",
    "env = dotenv_values(\".env\")\n",
    "\n",
    "OPENAI_API_KEY = env.get(\"OPENAI_API_KEY\")\n",
    "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Chain 1\n",
    "\n",
    "A basic LangChain setup where a prompt template is used with a language model to generate a joke based on a given topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Why did the Apple go to school?\\n\\nBecause it wanted to be a little smarter! üçèüìö', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 22, 'prompt_tokens': 13, 'total_tokens': 35, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_72ed7ab54c', 'finish_reason': 'stop', 'logprobs': None}, id='run-80512747-c4a7-45d0-b1d1-75de48804b04-0', usage_metadata={'input_tokens': 13, 'output_tokens': 22, 'total_tokens': 35, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "prompt = PromptTemplate.from_template(\"Tell me a joke about {topic}\")\n",
    "\n",
    "chain = prompt | llm\n",
    "chain.invoke({\"topic\": \"Apple\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Chain 2\n",
    "\n",
    "A LangChain setup with a chat-based prompt that guides the model to provide a solution for a given topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Absolutely! To help you create a LangChain chain for a LeetCode problem, we need to first clarify a few things:\\n\\n1. **Problem Description**: Please provide the specifics of the LeetCode problem you want to solve. This includes the problem statement, constraints, and any examples provided.\\n\\n2. **Desired Language/Framework**: Let me know if you have a preferred programming language (Python, JavaScript, etc.) or specific libraries you want to use along with LangChain.\\n\\n3. **Functionality**: Specify what you want the chain to do. Do you want the chain to generate Python code that solves the problem, test the solution, or explain the problem and its solution? \\n\\nOnce you provide that information, I can help craft a suitable LangChain chain tailored to your needs!', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 164, 'prompt_tokens': 27, 'total_tokens': 191, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_72ed7ab54c', 'finish_reason': 'stop', 'logprobs': None}, id='run-e2f2fa4d-38b2-4522-8557-91d347e9c942-0', usage_metadata={'input_tokens': 27, 'output_tokens': 164, 'total_tokens': 191, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are langchain expert\"),\n",
    "        (\"human\", \"Help me write a langchain chain for {topic}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "chain = prompt | llm\n",
    "chain.invoke({\"topic\": \"Leetcode problem\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Chain 3\n",
    "\n",
    "A LangChain setup that uses a chat-based prompt alongside a RunnablePassthrough to pass the output of the model directly for further processing or chaining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prompt': ChatPromptValue(messages=[SystemMessage(content='You are langchain expert', additional_kwargs={}, response_metadata={}), HumanMessage(content='Help me write a langchain chain for Leetcode problem', additional_kwargs={}, response_metadata={})])}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are langchain expert\"),\n",
    "        (\"human\", \"Help me write a langchain chain for {topic}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = prompt | {\"prompt\": RunnablePassthrough()}\n",
    "chain.invoke({\"topic\": \"Leetcode problem\"})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
